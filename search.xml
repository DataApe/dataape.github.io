<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Python爬虫之Scrapy框架安装配置]]></title>
      <url>%2F2017%2F03%2F10%2FPython%E7%88%AC%E8%99%AB%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[Windows 平台安装Scrapy之前确保已经安装好下列程序: 1. 安装Python]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python初级教程：入门详解]]></title>
      <url>%2F2017%2F03%2F09%2Fpython%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B%EF%BC%9A%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[文章转载，详情请见&gt;&gt;python入门 本文是针对Python的初学者，从无到有的介绍Python语言如何入门，主要包括了：Python的简介，如何下载Python，如何安装Python，如何使用终端、Shell，IDE等各种开发环境进行Python开发，Python中的语法和基本知识、概念和逻辑，以及继续深入学习Python的方法，另外还整理一些值得参考的资料。 安装python1. Python官网不是太稳定，对于国内开发者来说，时不时会出现无法访问的情况。可以选择国内的镜像进行下载。 官网上的下载地址 国内的镜像 http://www.python.org/ftp/python/http://python.org/getit/releases/ http://mirrors.sohu.com/python/ 2. Python有2.x版本和3.x版本的区别。对于初学者，推荐先用Python 2.x版本的。 总结Python2(Python 2.x版本）和Python3（Python 3.x版本）之间的区别 如何在Windows系统中安装Python 如何选用Python的开发环境 针对初学者，建议使用windows的cmd下，去运行python脚本。目的很明确：很多东西的学习，其本质上，都是需要一个循序渐进的过程的，学习Python语言同样如此。在没有学会走路，即如何搞懂Windows的cmd下运行Python脚本，就想学会跑了，即直接利用Python的IDE，包括shell和第三方开发环境，结果就是，很多东西，还是不明白到底是为什么，理解的不透彻。而当Python的基本知识，基本开发流程熟悉了之后，再建议你去使用第三方的Python的IDE。 先：Windows的cmd + Notepad++再：选用某个IDE，比如PyScripter，Ulipad，Eclipse+PyDev等。 问题 在window的cmd中运行python结果却调用了文本编辑器去打开了，而不是去调用Python解析器去运行python文件 解决办法：修改后缀为.py文件的打开方式，设置为python]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分类和预测]]></title>
      <url>%2F2017%2F03%2F03%2F%E5%88%86%E7%B1%BB%E5%92%8C%E9%A2%84%E6%B5%8B%2F</url>
      <content type="text"><![CDATA[分类：预测类对象的分类标号，根据训练数据集和类标号属性，构建模型来分类现有数据，并用来分类新数据。银行需要弄清楚那些贷款申请者是安全的，哪些是有风险的。预测：建立连续函数值模型，比如预测空缺值，或者预测顾客在某项活动中的花费。银行需要预测贷给某个顾客多少钱是安全的，预测一个连续值函数或有序值，常用方法是回归分析。 典型应用：欺诈检测 市场定位 医疗诊断 准备分类和预测的数据数据清洗 修正错误值 弥补缺失值 处理极端值 相关分析 有些属性可能是冗余的，使用相关分析来识别任意两个给定的属性是否是统计相关的 属性子集选择通过删除不相关或冗余的属性减少数据量。属性子集选择的目标是找出最小属性集，使得数据类的概率分布尽可能地接近使用所有属性得到的原分布。 向前选择 向后删除 决策树归纳 初始属性集：{A1，A2，A3，A4，A5，A6}初始化归约集：{ }=&gt; {A1}=&gt; {A1，A4}=&gt; 归约后的属性集：{A1，A4，A6} 初始属性集：{A1，A2，A3，A4，A5，A6}=&gt; {A1，A3，A4，A5，A6} =&gt; {A1，A4，A5，A6}=&gt; 归约后的属性集：{A1，A4，A6} 初始属性集：{A1，A2，A3，A4，A5，A6}当决策树归纳用于属性子集选择时，由给定的数据构造决策树。不出现在树中的所有属性假定是不相关的。出现在树中的属性形成归约后的属性子集。 这些方法的结束条件可以不同。该过程可以使用一个度量阈值来决定何时停止属性选择过程。 数据变换和归约 字符型（character）变量转换数值型（numeric）变量；从原始数据中提炼出大量的衍生变量，根据一定的统计原理把数据单位或变量标准化。 规范化涉及将所给属性的所有值按比例进行缩放，使其落入一个较小的指定区间，例[-1.0,1.0]或者[0.0,1.0] 数据也可以通过泛化到较高层概念进行变换。例如，属性income的数值可以泛化为离散的区间，如low，medium和high。泛化可以压缩原来的训练数据，使得学习时的输入输出得以减少。 其他归约的办法如：小波变换，主成分分析，分箱，直方图分析，聚类的离散化技术 分类办法决策树 决策树构建，使用属性选择度量来选择将元组最好的划分为不同的类的属性，递归的通过选定的属性，来划分样本（必须是离散值） 树剪枝，决策树建立时，许多分枝反映的是训练数据中的噪声和离群点，树剪枝试图识别并减去这种分枝，以提高对未知数据分类的准确性 提取分类规则，从根到树叶的每条路径创建一个规则，并以“IF-THEN”形式的分类规则表示。示例： IF age=”youth” AND student=”no” THEN buys_computer=”no” 属性选择度量是一种选择分裂准则，将给定类标号的训练元组最好的进行划分的方法，常用的有：① 信息增益；② 增益率；③ Gini 指标。 贝叶斯分类 比较分类办法使用下列标准比较分类和预测方法 分类器的准确率度量（Accuracy）：模型正确预测新数据的类编号的能力 分类器的灵敏性（Sensitivity）度量和特效性（Specificity）度量 假设已经训练的分类器将医疗数据元组分类为“cancer”和“not_cancer”。90%的准确率使该分类器看上去相当准确，但是，如果只有 3%~4%的训练元组是“cancer”，显然90%的准确率是不可以接受的（比如该分类器只能对“not_cancer”类的元组正确分类，对 “cancer”类的元组全部分类错误）。我们希望有某种度量能够对分类器识别“cancer”元组（设为正元组）和“not_cancer”元组（设为负元组）进行分别评估，为此引入灵敏性（Sensitivity）度量和特效性（Specificity）度量。 速度：产生和使用模型的计算花销 健壮性：给定噪声数据或有空缺值的数据，模型正确预测的能力 可伸缩性：对大量数据，有效的构建分类器或预测器的能力 可解释性：学习模型提供的理解和洞察的层次 过度拟合（Overfit）问题 Overfit是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就 叫这个假设出现了overfit的现象。 过度拟合产生的原因 噪声数据导致过度拟合由于数据中的噪声和孤立点，许多分枝反应的是训练数据中的异常。 缺乏代表性样本导致过度拟合在训练数据缺乏具有代表的样本的情况下，往往需要继续细化模型才能得到较好拟合训练集的模型，这样得到的模型同样可能具有较高的泛化误差。我们将训练好的模型检验训练集数据，得到的误差率称之为训练误差。将该模型用于检验测试样本，得到的误差率称之为泛化误差。 避免过度拟合的策略与上述两个导致模型过度拟合的因素同时出现的是模型的复杂度。模型越复杂出现过度拟合的概率就越高。因此，对于给定具有相同泛化误差的模型，我们往往更倾向于较为简单的模型，这就是所谓的Occam剃刀（Occam’s Razor）原则。为了避免过度拟合，降低决策树的复杂度，通常的策略是剪枝，该策略采用统计方法删除最不可靠的分支，以提高对未来分类识别的速度和分类 识别新数据的能力。 先剪枝（pre-pruning）法先剪枝法是通过提前停止分支的生长过程，即通过在当前结点上就判断是否需要继续划分该结点所包含训练样本集来实现的。这就需要更为限制性的约束条件，如当观察到衡量不纯性的度量低于某个阈值就停止分支的生长。该方法的优点在于避免产生过分拟合训练数据的过于复杂的子树。但是我们很难为提前终止选择正确的阈值，阈值太高将导致拟合不足，阈值太低则不能充分解决过度拟合问题。 后剪枝（post-pruning）法后剪枝法从一个“充分生长”的树中，按照自底向上的方式修剪多余的分支。对于树中的每个非树叶节点，计算该节点上的子树被剪枝可能出现的期望错误率。使用一个独立的测试集来评估每棵树的准确率，就能得到具有最小期望错误率的决策树。① 用新的叶子节点替换子树，该叶子节点的类标签由子树记录中的多数类确定；② 用子树中最常用的分支代替子树。 J48决策树算法采用了子树提升（Subtree Raising）与子树替换（Subtree Replacement）的修剪策略。计算修剪前后的预期分类错误率，如果修剪导致预期分类错误率变大，则放弃剪枝，保留该结点的相应分支，否则就将相应节点分支修剪删除。在产生一系列经过修剪的决策树候选之后，利用一个独立的测试数据集对这些经过修剪的决策树的分类的准确性进行评价，保留下那些预期分类错误率最小的决策树。与先剪枝相比，后剪枝倾向于产生更好的结果 检验和解释结果在实际问题中，数据往往有许多噪音，所以必须对统计结果进行显著性检验、准确性检验的基础上，对结果进行经验性的解释，已确定结果hi合乎逻辑、合乎情理、合乎直觉的，是能够解决实际问题的，而不是噪音、数据偏差（bias）或过拟合（overfit）的结果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F15%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
